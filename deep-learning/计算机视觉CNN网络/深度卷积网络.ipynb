{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几种经典的架构\n",
    "\n",
    "+ LeNet-5（针对灰度图像）\n",
    "\n",
    "![001](images_02/001.png)\n",
    "\n",
    "    1.随着网络层数的叠加，图像一直被缩小\n",
    "    \n",
    "    2.信道不断增加\n",
    "    \n",
    "    3.池化层采用了平均法。\n",
    "    \n",
    "**每个过滤器的信道数和他的上一层的信道数相同,然后由本层的过滤器数量决定下一层的信道数**\n",
    "\n",
    "+ AlexNet\n",
    "![001](images_02/002.png)\n",
    "\n",
    "    1.采用了relu\n",
    "    \n",
    "    2.有6000万参数\n",
    "\n",
    "+ VGG\n",
    "![001](images_02/003.png)\n",
    "\n",
    "    1.1.38亿个参数\n",
    "    \n",
    "    2.模型比较有规则，都是经过几个卷积层时再进行池化，缩小图像的高度和宽度。图像缩小和信道增加是有规律的。\n",
    "    \n",
    "\n",
    "### ResNet(残差网络 Residual block)\n",
    "\n",
    "Residual block（残差块）\n",
    "\n",
    "![005](images_02/005.png)\n",
    "\n",
    "![004](images_02/004.png)\n",
    "\n",
    "残差网络为什么有用？\n",
    "\n",
    "一个网络深度越深，他在训练集上训练网络的效率会有所减弱。\n",
    "\n",
    "![004](images_02/006.png)\n",
    "\n",
    "$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$$\n",
    "\n",
    "添加项$a^{[l]}$是刚添加的跳远连接的输入。\n",
    "\n",
    "![007](images_02/007.png)\n",
    "\n",
    "在深层的神经网络增加两层，无论把残差快添加到神经网络的中间还是末端位置，都不会影响网络的表现。\n",
    "\n",
    "主要原因：\n",
    "\n",
    "相对与普通的深层网络，残差网络学习恒等函数非常容易，可以保证后面的网络效率不会降低，很多时候甚至可以提高效率。\n",
    "\n",
    "### 网络中的网络(1*1)：\n",
    "\n",
    "![009](images_02/009.png)\n",
    "\n",
    "1*1的网络可以有效压缩$n_c$,即输入数据的维度，信道数的维度。\n",
    "然而对于池化层，我们职能压缩这些层的高度和宽度。\n",
    "\n",
    "### Inception\n",
    "\n",
    "    作用：代替人工来确定卷积层中的过滤器，或者确定是否需要创建卷积层或池化层。\n",
    "  \n",
    "![010](images_02/010.png)\n",
    "\n",
    "  模型自己调节采用什么过滤器组合。\n",
    "  \n",
    "**计算成本：**\n",
    "\n",
    "![011](images_02/011.png)\n",
    "\n",
    "\n",
    "![012](images_02/012.png)\n",
    "\n",
    "* 通过加入1*1的卷积层，先缩小原图（合理构建瓶颈层），然后执行卷积操作最后得到结果，可以有效地降低计算成本。\n",
    "\n",
    "构建inception模型：\n",
    "\n",
    "![014](images_02/014.png)\n",
    "\n",
    "![015](images_02/015.png)\n",
    "\n",
    "**其中包括inception模块**\n",
    "\n",
    "![016](images_02/016.png)\n",
    "\n",
    "inception的中间件或隐藏单元也能参与预测分类，中间层加入全连接和softmax函数的好处就是可以防止过拟合。\n",
    "\n",
    "\n",
    "## 迁移学习\n",
    "\n",
    "下载别人训练好的模型和参数，先报存一部分参数不做改变，通过训练后续的参数，得到自己的模型。\n",
    "\n",
    "![017](images_02/017.png)\n",
    "\n",
    "## 数据增强\n",
    "\n",
    "+ 1.镜像对称，反转，裁剪等\n",
    "\n",
    "![018](images_02/018.png)\n",
    "\n",
    "+ 2.颜色变换\n",
    "\n",
    "![019](images_02/019.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅层神经网络\n",
    "\n",
    "将多个类似的logistic回归单元叠加在一起，初步形成简单的神经网络。\n",
    "\n",
    "![029](images/029.png)\n",
    "\n",
    "多次计算$z,a$,最后计算损失函数。\n",
    "\n",
    "![030](images/030.png)\n",
    "\n",
    "隐藏层的意思就是实现过程中对数据的不可见。\n",
    "\n",
    "\n",
    "![031](images/031.png)\n",
    "\n",
    "$w^{[1]},b^{[1]}$表示第一层的参数和偏置。\n",
    "\n",
    "计算简单神经网络的输出：\n",
    "\n",
    "![032](images/032.png)\n",
    "\n",
    "\n",
    "![033](images/033.png)\n",
    "\n",
    "\n",
    "使用代码实现上述公式。\n",
    "\n",
    "![034](images/034.png)\n",
    "\n",
    "上述是循环遍历所有的样本。\n",
    "\n",
    "![035](images/035.png)\n",
    "\n",
    "向量化解决遍历问题，横向代表了所有的样本，竖向代表了样本的特征值。\n",
    "\n",
    "例子：\n",
    "\n",
    "列堆列的输入，对应列对应的输出。\n",
    "\n",
    "![036](images/036.png)\n",
    "\n",
    "![037](images/037.png)\n",
    "\n",
    "## 如何选择激活函数\n",
    "\n",
    "\n",
    "\n",
    "+ sigmoid函数$${\\sigma}=\\frac{1}{1+e^{-z}}$$，界限是(0,1)除非用于二元分类的输出层，不然绝对不要用。\n",
    "\n",
    "\n",
    "\n",
    "+ tanh函数  $$tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$\n",
    "\n",
    " 是sigmoid函数的变换，界限是{-1，1}，使用tanh(x)有数据中心化的感觉，使得数据的平均值接近于0.\n",
    " \n",
    " **tanh(x)几乎适用于所有的场合，总是优于sigmoid函数，但是当输出在(0,1)之间时，一般使用sigmoid函数。不同层可以使用不同的激活函数**\n",
    "\n",
    "\n",
    "缺点：\n",
    "   当x取很大或很小的值时，函数的梯度会减少，会减缓学习的效率和精度。\n",
    "\n",
    "+ Relu（修正线性单元）$$a=max(0,z)$$\n",
    "\n",
    "    只要z为正，导数就是1.\n",
    "    \n",
    "   ![038](images/038.png)\n",
    "\n",
    " 缺点：\n",
    " 值为负值时，导数是零。\n",
    " \n",
    " 通常情况下，Relu的使用效果比sigmoid和tanh都好，速度也很快，主要原因是Relu没有这种函数斜率接近与0时的情况。\n",
    " \n",
    " + leaky-Relu激活函数$$a=max(0.01z,z)$$\n",
    " \n",
    " ![040](images/040.png)\n",
    " \n",
    " 为什么要使用非线性激活函数？\n",
    " \n",
    " ![041](images/041.png)\n",
    " \n",
    " 线性激活函数就是输入的线性组合再输出，没有实际意义。\n",
    " \n",
    " \n",
    " 只有一个地方可以使用线性组合，就是机器学习的线性回归。\n",
    "    \n",
    "\n",
    "常见激活函数的导数：\n",
    "\n",
    "+ sigmoid函数\n",
    "\n",
    "![042](images/042.png)\n",
    "\n",
    "+ tanh(x)函数\n",
    "\n",
    "![043](images/043.png)\n",
    "\n",
    "+ Relu函数\n",
    "\n",
    "![044](images/044.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x,deriv=False):\n",
    "    if (der==False):\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "    else:\n",
    "        return (1.0/(1.0+np.exp(-x)))*(1-(1.0/(1.0+np.exp(-x))))\n",
    "    \n",
    "def tanh(x,deriv=False):\n",
    "    if(deriv==False):\n",
    "        return (1.0-np.exp(-2*x))/(1.0+np.exp(-2*x))\n",
    "    else:\n",
    "        return 1.0-((1.0-np.exp(-2*x))/(1.0+np.exp(-2*x))**2)\n",
    "    \n",
    "def relu(x,deriv=False):\n",
    "    if(deriv==False):\n",
    "        return np.where(x<0,0,x)\n",
    "    else:\n",
    "        return np.where(x<0,0,1)\n",
    "def l_relu():\n",
    "    if(deriv==False):\n",
    "        return np.where(x<0,0.01x,x)\n",
    "    else:\n",
    "        return np.where(x<0,0.01,1)\n",
    "\n",
    "def softmax(x):\n",
    "    x_exp=np.exp(x)\n",
    "    x_sum=np.sum(x_exp, axis=1, keepdims=True)\n",
    "    s = x_exp / x_sum\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是softmax函数的广播过程：\n",
    "![045](images/045.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 正则化函数\n",
    "\n",
    "# L1损失函数\n",
    "import numpy as np\n",
    "def l1(yhat,y):\n",
    "    loss=np.sum(abs(y-yhat))\n",
    "    return loss\n",
    "\n",
    "# L2损失函数\n",
    "\n",
    "def L2(yhat,y):\n",
    "    loss=np.sum(np.multiply((y-yhat),(y-yhat)))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的梯度下降算法\n",
    "\n",
    "![046](images/046.png)\n",
    "\n",
    "正向传播：\n",
    "\n",
    "$$Z^{[1]}=W^{[1]}X+b^{[1]}  ....第一层$$\n",
    "\n",
    "$$A{[1]}=g^{[1]}(Z^{[1]})$$\n",
    "\n",
    "$$Z^{[2]}=W^{[2]}A{[1]}+b^{[2]}  ...第二层$$\n",
    "\n",
    "$$A^{[2]}=g^{[2]}(Z^{[2]})$$\n",
    "\n",
    "反向传播：\n",
    "\n",
    "$$dZ^{[2]}=A^{[2]}-Y       ....(1,m)维 $$\n",
    "\n",
    "$$dW^{[2]}=\\frac{1}{m}*{dZ^{[2]}{A^{[1]}}^T}$$\n",
    "\n",
    "$$db^{[2]}=\\frac{1}{m}*np.sum(dZ^{[2]},axis=1,keepdims=True)$$\n",
    "\n",
    "$$dZ^{[1]}={W^{[2]}}^T*dZ^{[2]}*{g^{[1]}}^`(Z^{[1]})    第二个乘是点乘$$\n",
    "\n",
    "$$dW^{[1]}=\\frac{1}{m}dZ^{[1]}X^T$$\n",
    "\n",
    "$$db^{[1]}=\\frac{1}{m}*np.sum(dZ^{[1]},axis=1,keepdims=True)$$\n",
    "\n",
    "\n",
    "推导过程：\n",
    "\n",
    "![048](images/048.png)\n",
    "\n",
    "## 随机初始化\n",
    "\n",
    "\n",
    "![049](images/049.png)\n",
    "\n",
    "一般初始化参数时选择比较小的参数。通常采用np.random.randn((n,m))*0.01等方法，保证sigmoid或者tanh的梯度较大，保证学习的效率。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.训练集，验证集，测试集\n",
    "\n",
    "![001](images/001.png)\n",
    "\n",
    "按照数据规模的大小，划分train，dev（验证模型算法），test集合。\n",
    "\n",
    "![002](images/002.png)\n",
    "\n",
    "要确保训练集和测试集来自同一个分布。\n",
    "\n",
    "## 2.偏差和方差\n",
    "\n",
    "![003](images/003.png)\n",
    "\n",
    "理解方差和偏差的两个关键数据就是训练集误差和验证集误差。\n",
    "\n",
    "+ 通过观察训练集误差和验证集误差，便可以诊断算法的是否具有高方差。\n",
    "\n",
    "    1.过拟合    训练集高（1%），验证集低（11%）    高方差\n",
    "    \n",
    "    2.欠拟合    训练集高（15%），验证集高（16%）    高偏差\n",
    "    \n",
    "    3.效果都不佳  训练集高（15%），验证集也高（30%）   高方差，高偏差\n",
    "    \n",
    "    4.效果佳    训练集（0.5%），验证集（1%）    低方差，低偏差\n",
    "\n",
    "## 3.机器学习基础——偏差和方差\n",
    "\n",
    "**在机器学习中的做法？**\n",
    "\n",
    "+ 1.高偏差（训练集误差）      ---改变模型（神经元个数，隐藏层数量等），尝试更先进的优化算法，更多的网络架构\n",
    "\n",
    "+ 2.当偏差降到了一定的程度，考虑方差（验证集检验） ---1.增加训练数据（过拟合）2.正则化处理 3.尝试不同的模型。\n",
    "\n",
    "我们通常会使用训练验证集来诊断算法是否存在偏差和方差的问题。\n",
    " \n",
    "![004](images/004.png)\n",
    "\n",
    "## 4.正则化\n",
    "\n",
    "通过适当的提高偏差来提高方差。\n",
    "\n",
    "解决高方差问题的方法：\n",
    "\n",
    "+ 1.正则化\n",
    "\n",
    "+ 2.增加数据集，训练集等大规模的数据\n",
    "\n",
    "![005](images/006.png)\n",
    "\n",
    "我们通常使用的${\\lambda}$正则化参数是使用验证集和交叉验证集来配置。\n",
    "\n",
    "L1，L2正则项。\n",
    "\n",
    "神经网络中的正则化(权重衰减)：\n",
    "\n",
    "![007](images/007.png)\n",
    "\n",
    "\n",
    "![009](images/009.png)\n",
    "\n",
    "### 为什么正则化可以防止过拟合？\n",
    "\n",
    "\n",
    "正则化通过减小参数来预防函数出现过拟合的状态。\n",
    "\n",
    "![009](images/010.png)\n",
    "\n",
    "### 4.2Dropout正则化（过拟合的解决办法）\n",
    "\n",
    "dropout会遍历神经网络的每一层，并设置消除神经网络中节点的概率，消除网络中的节点，得到精简的神经网络。\n",
    "\n",
    "dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。\n",
    "\n",
    "+ 1.反向随机失活（inverted dropout）\n",
    "\n",
    "![009](images/011.png)\n",
    "\n",
    "如何在测试阶段训练算法：\n",
    "\n",
    "![009](images/012.png)\n",
    "\n",
    "测试阶段不使用dropout，多次测试的结果都差不多，每次预测都会有随机的隐层单元被置零，但是计算效率低，得出的结果几乎相同。\n",
    "\n",
    "### 4.3其他正则化方法\n",
    "\n",
    "+ 1.数据扩增\n",
    "\n",
    "![009](images/013.png)\n",
    "\n",
    "数据扩增可以作为正则化方法使用。\n",
    "\n",
    "+ 2.Early stopping（提早停止训练神经网络，当验证集上的效果变差的时候）\n",
    "\n",
    "![009](images/014.png)\n",
    "\n",
    "根据验证集上的错误率曲线，Early stopping就是在中间停止迭代，得到一个w值中等大小的弗罗贝尼乌斯范数，和L2正则化相似，选择参数w范数较小的神经网络，保证模型不会过拟合。\n",
    "\n",
    "## 正则化输入（加速训练过程）\n",
    "\n",
    "+ 1.归一化\n",
    "      归一化需要两个步骤：\n",
    "      1.零均值化\n",
    "$${\\mu}=\\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}$$\n",
    "$$x:=x-{\\mu}$$\n",
    "\n",
    "      2.归一化方差\n",
    "      \n",
    "$${\\sigma}^{2}=\\frac{1}{m}\\sum_{i=1}^{m}{x^{(i)}}^2,其中x是上一步的x向量$$\n",
    "$$x=\\frac{x}{{\\sigma}^{2}}$$\n",
    "\n",
    "![009](images/015.png)\n",
    "\n",
    "**需要使用相同的方法调整测试集***\n",
    "\n",
    "![009](images/016.png)\n",
    "\n",
    "优化成本函数时比较方便\n",
    "\n",
    "\n",
    "## 梯度消失和梯度爆炸\n",
    "\n",
    " 训练神经网络，尤其是深度神经网络所面临的问题，训练过程中，导数和坡度又是会变得非常大，或非常小，甚至是以指数方式变小，这就加大了训练的难度。\n",
    " \n",
    " ![017](images/017.png)\n",
    " \n",
    " ### 通过合理的初始化网络权重缓解上述问题\n",
    " \n",
    " \n",
    " 帮助神经网络更好的选择参数，有效缓解梯度消失和梯度爆炸。\n",
    " \n",
    "![018](images/018.png)\n",
    "\n",
    "激活函数是$relu$,则$w^{[l]}=np.random.randn(shape)*np.sqrt(\\frac{2}{n^{[l-1]}})$\n",
    "\n",
    "激活函数是$tanh$,则$w^{[l]}=np.random.randn(shape)*np.sqrt(\\frac{1}{n^{[l-1]}})$\n",
    "\n",
    "## 梯度检验\n",
    "\n",
    "通过检测梯度来发现网络中的一些bug。\n",
    "\n",
    "![018](images/019.png)\n",
    "\n",
    "![018](images/020.png)\n",
    "\n",
    "\n",
    "如何在神经网络中实施梯度检验的使用技巧和注意事项：\n",
    "\n",
    "+ 1.不要在训练中使用梯度检验，他只是用于调试\n",
    "\n",
    "+ 2.如果算法的梯度检验失败，要检查所有项\n",
    "\n",
    "+ 3.正则化的使用\n",
    "\n",
    "+ 4.不能和dropout一起使用\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
